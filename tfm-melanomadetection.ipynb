{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Melanoma Classification\n\nKaggle Competition Page: www.kaggle.com/c/siim-isic-melanoma-classification/overview\n\n\n## What is Melanoma?\nMelanoma, the most severe type of skin cancer, develops in the cells (melanocytes) that produce melanin — the pigment that gives your skin its color. Melanoma can also form in your eyes and, rarely, inside your body, such as in your nose or throat.\n\nThe exact cause of all melanomas isn't clear, but exposure to ultraviolet (UV) radiation from sunlight or tanning lamps and beds increases your risk of developing melanoma.\n\nThe risk of melanoma seems to be increasing in people under 40, especially women. Knowing the warning signs of skin cancer can help ensure that cancerous changes are detected and treated before the cancer has spread. We can treat melanoma successfully if it is detected early.","metadata":{"id":"XvBDTao8djYJ"}},{"cell_type":"markdown","source":"<img src=\"https://github.com/SaschaMet/melanoma-classification/blob/master/images/melanoma.jpg?raw=1\" alt=\"Drawing\" style=\"width: 600px;\"/>","metadata":{"id":"PQIr6OLjdjYs"}},{"cell_type":"markdown","source":"## Symptoms & Diagnosis\nMelanomas can develop anywhere on your body. They most often develop in areas with exposure to the sun, such as your back, legs, arms, and face.\nMelanomas can also occur in areas that don't receive much sun exposure, such as the soles of your feet, palms of your hands, and fingernail beds. These hidden melanomas are more common in people with darker skin.\n\nTo help you identify characteristics of melanomas or other skin cancers, think of the letters ABCDE:\n- A is for asymmetrical shape. Look for moles with irregular shapes, such as two very different-looking halves.\n- B is for irregular border. Look for moles with rough, notched, or scalloped edges — characteristics of melanomas.\n- C is for color changes. Look for growths that have many colors or an uneven distribution of color.\n- D is for diameter. Look for new growth in a mole larger than 1/4 inch (about 6 millimeters).\n- E is for evolving. Look for changes over time, such as a mole that grows in size or changes color or shape.\n","metadata":{"id":"wR5V4LGwdjYs"}},{"cell_type":"markdown","source":"![ABCDE Melanoma](https://github.com/SaschaMet/melanoma-classification/blob/master/images/abcde-melanoma.jpg?raw=1)\n\nSource: https://www.health.harvard.edu/cancer/melanoma-overview","metadata":{"id":"3_Vl79qwdjYw"}},{"cell_type":"markdown","source":"The facts about Melanoma:\n- Melanoma is the most severe form of skin cancer\n- It makes up 2% of skin cancers but is responsible for 75% of skin cancer deaths\n- Australia and New Zealand have the highest melanoma rates in the world\n- 1 in 17 Australians will be diagnosed with melanoma before the age of 85\n- More than 90% of melanoma can be successfully treated with surgery if detected early\n\nSource: https://melanomapatients.org.au/about-melanoma/melanoma-facts/","metadata":{"id":"Bm8mD1YSdjYx"}},{"cell_type":"markdown","source":"<img src=\"https://github.com/SaschaMet/melanoma-classification/blob/master/images/melanoma-impact.jpg?raw=1\" alt=\"Drawing\" style=\"width: 600px;\"/>\n\nSource: https://impactmelanoma.org/wp-content/uploads/2018/11/Standard-Infographic_0.jpg","metadata":{"id":"si5Twho8djYx"}},{"cell_type":"markdown","source":"## Setup","metadata":{"id":"e-AFobcxdjYx"}},{"cell_type":"code","source":"import os\nimport glob\nimport shutil\n\nfiles = glob.glob('/kaggle/working/*')\nfor f in files:\n    try:\n        os.remove(f)\n    except:\n        shutil.rmtree(f)","metadata":{"execution":{"iopub.status.busy":"2021-08-22T07:57:43.300656Z","iopub.execute_input":"2021-08-22T07:57:43.301009Z","iopub.status.idle":"2021-08-22T07:57:43.308645Z","shell.execute_reply.started":"2021-08-22T07:57:43.300976Z","shell.execute_reply":"2021-08-22T07:57:43.307419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport random\nfrom pathlib import Path\nimport pandas as pd\nimport tensorflow as tf\nimport numpy as np\nfrom keras.preprocessing.image import ImageDataGenerator\nimport cv2\nimport matplotlib.pyplot as plt\nfrom keras.applications.vgg16 import VGG16\nfrom keras.applications.densenet import DenseNet121\nfrom keras.applications.nasnet import NASNetMobile, NASNetLarge\nfrom keras.applications.inception_resnet_v2 import InceptionResNetV2\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras import layers\nfrom datetime import datetime, date\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import class_weight\nfrom keras.optimizers import Adam\nimport json\nfrom sklearn.metrics import roc_curve, auc, precision_recall_curve, plot_precision_recall_curve, confusion_matrix\nfrom tensorflow import keras\nimport itertools\nfrom tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2021-08-24T12:23:40.616968Z","iopub.execute_input":"2021-08-24T12:23:40.617466Z","iopub.status.idle":"2021-08-24T12:23:40.626416Z","shell.execute_reply.started":"2021-08-24T12:23:40.617422Z","shell.execute_reply":"2021-08-24T12:23:40.625466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\">\n<b>Tip:</b> Use blue boxes (alert-info) for tips and notes. \nIf it’s a note, you don’t have to include the word “Note”.\n</div>","metadata":{}},{"cell_type":"code","source":"SEED = 1\nPIXELS_SIZE = 224\nIMG_SIZE = (PIXELS_SIZE, PIXELS_SIZE)\nINPUT_SHAPE = (PIXELS_SIZE, PIXELS_SIZE, 3)\nOUTPUT_NEURONS = 1\nVERBOSE_LEVEL = 2\nSAVE_OUTPUT = True\nCLASS_MODE = \"binary\"\n# \"raw\" evitarlo y \"categorical\" debería de ser igual pero al ser onehot pues también evitarlo en principio\nIS_CLASS_MODE_BINARY = CLASS_MODE == \"binary\"\nPOSITIVE_CLASS = \"1\" if IS_CLASS_MODE_BINARY else 1\nNEGATIVE_CLASS = \"0\" if IS_CLASS_MODE_BINARY else 0\n\nFAST_RUN = True\n\nDO_UNDERSAMPLING = True\nPREPRO_ROTATION = True\nPREPRO_BLUR = True\nPREPRO_BRIGHTNESS = True\nPREPRO_ZOOM = True\nPREPRO_BRIGHTNESS_LOW = 0.75 if PREPRO_BRIGHTNESS else 1\nPREPRO_ZOOM_LOW = 0.75 if PREPRO_ZOOM else 1\n\nBATCH_SIZE = 64 if DO_UNDERSAMPLING else 256\nEPOCHS = 40 if DO_UNDERSAMPLING else 100\n\nBASE_MODEL_TRAINABLE = False\n\nbase_model = NASNetMobile(\n    input_shape=INPUT_SHAPE,\n    include_top=False,\n    weights='imagenet'\n)\n\nLEARNING_RATE = 1e-4 if not BASE_MODEL_TRAINABLE else 1e-5 #Probar también con 1e-5 si acaso\nOPTIMIZER = Adam(lr=LEARNING_RATE) #Dejar esto que es el que mejor funcionar\nLOSS = 'binary_crossentropy'\nMETRICS = [\n    'accuracy', \n    'AUC'\n] \n\ntimestamp = str(date.today()) + \"_\" + str(datetime.now().strftime(\"%H:%M:%S\"))\n\nexperiment_id = \"NASNetMobile_\"+ (\"Under_\" if DO_UNDERSAMPLING else \"CW_\") + \"Pre\"+ (\"Rot\" if PREPRO_ROTATION else \"\")+ (\"Blur\" if PREPRO_BLUR else \"\")\nexperiment_id = experiment_id + (\"Bright\" if PREPRO_BRIGHTNESS else \"\") + (\"Zoom\" if PREPRO_ZOOM else \"\")\nexperiment_id = experiment_id +\"_\"+ str(BATCH_SIZE) + \"B_\" + str(EPOCHS) + \"E_\" + \"LR\" + str(LEARNING_RATE)\nexperiment_id = experiment_id +\"_\" + (\"FineTuning\" if BASE_MODEL_TRAINABLE else \"Extractor\")\nbase_output = \"./\" +experiment_id+\"/\"\nbase_output_path = base_output + experiment_id + \"-\"\ntry:\n    os.makedirs(experiment_id)\n    open(\"./\" +experiment_id+\"/\" + timestamp, 'w').close()\nexcept FileExistsError:\n    pass\n\n\nBASE_PATH = '/kaggle/input/tfmmelanomapreprocessed'\nPATH_TO_IMAGES = '/kaggle/input/tfmmelanomapreprocessed/dataset/jpeg'+str(PIXELS_SIZE)\nIMAGE_TYPE = \".jpg\"\n\n# warnings.filterwarnings('ignore')\n    \n# Tensorflow execution optimizations\n# Source: https://www.tensorflow.org/guide/mixed_precision & https://www.tensorflow.org/xla\nMIXED_PRECISION = True\nXLA_ACCELERATE = True\nGPUS = 0\n\nGPUS = len(tf.config.experimental.list_physical_devices('GPU'))\nif GPUS == 0:\n    DEVICE = 'CPU'\n    raise RuntimeError('Running on CPU')\nelse:\n    DEVICE = 'GPU'\n    if MIXED_PRECISION:\n        from tensorflow.keras.mixed_precision import experimental as mixed_precision\n        policy = tf.keras.mixed_precision.experimental.Policy('mixed_float16')\n        mixed_precision.set_policy(policy)\n        print('Mixed precision enabled')\n    if XLA_ACCELERATE:\n        tf.config.optimizer.set_jit(True)\n        print('Accelerated Linear Algebra enabled')\n\nprint(\"Tensorflow version \" + tf.__version__)\n\n\nprint(\"Set seeds\")\nrandom.seed(SEED)\nnp.random.seed(SEED)\nos.environ['PYTHONHASHSEED'] = str(SEED)\nos.environ['TF_KERAS'] = str(SEED)\nos.environ['TF_DETERMINISTIC_OPS'] = str(SEED)\nos.environ['TF_CUDNN_DETERMINISTIC'] = str(SEED)\ntf.random.set_seed(SEED)","metadata":{"id":"JnvR9SrjdjYy","outputId":"1bc1ff6f-760d-4cff-bfd8-9885ae28ebb7","execution":{"iopub.status.busy":"2021-08-22T07:57:43.324079Z","iopub.execute_input":"2021-08-22T07:57:43.324576Z","iopub.status.idle":"2021-08-22T07:57:53.510148Z","shell.execute_reply.started":"2021-08-22T07:57:43.324538Z","shell.execute_reply":"2021-08-22T07:57:53.509154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loading the data (csv files)","metadata":{"id":"E7Vq9jDVdjY3"}},{"cell_type":"code","source":"def check_image(fileName, folder):\n    absolutePath = PATH_TO_IMAGES + folder + fileName + IMAGE_TYPE\n    img_file = Path(absolutePath)\n    if img_file.is_file():\n        return absolutePath\n    return False\n\ndef get_train_data():\n    print(\"Loading train data\")\n    train = pd.read_csv(os.path.join(BASE_PATH, 'train.csv'))\n    train['image_path'] = train['image_name'].apply(lambda x: check_image(x, \"/train/\"))\n    train = train[train['image_path'] != False]\n    print(\"valid rows in train\", train.shape[0])\n    return train\n\ndef get_test_data():\n    print(\"Loading test data\")\n    test = pd.read_csv(os.path.join(BASE_PATH, 'test.csv'))\n    test['image_path'] = test['image_name'].apply(lambda x: check_image(x, \"/test/\"))\n    test = test[test['image_path'] != False]\n    print(\"valid rows in test\", test.shape[0])\n    return test\n\ntrain = train_backup.copy() if \"train_backup\" in globals() else get_train_data()\ntest = test_backup.copy() if \"test_backup\" in globals() else get_test_data()\ntrain_backup = train.copy()\ntest_backup = test.copy()\n\nprint(train.dtypes)\nprint(test.dtypes)","metadata":{"execution":{"iopub.status.busy":"2021-08-22T07:57:53.515014Z","iopub.execute_input":"2021-08-22T07:57:53.515378Z","iopub.status.idle":"2021-08-22T07:59:56.298416Z","shell.execute_reply.started":"2021-08-22T07:57:53.515339Z","shell.execute_reply":"2021-08-22T07:59:56.297481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Train Dataset:\n- image name: the filename for the specific image\n- patient_id: unique patient id\n- sex: gender of the patient\n- age_approx: age of the patient\n- anatom_site_general_challenge: location of the scan site\n- diagnosis: information about the diagnosis\n- benign_malignant: indicates if the scan result is malignant or benign\n- target: 0 for benign and 1 for malignant\n- image_path: path to the image\n\nTest Dataset Consists Of:\n- image name: the filename for the specific image\n- patient_id: unique patient id\n- sex: gender of the patient\n- age_approx: age of the patient\n- anatom_site_general_challenge: location of the scan site\n- image_path: path to the image","metadata":{"id":"YPt_jwVidjY5"}},{"cell_type":"markdown","source":"## Data preparation","metadata":{"id":"NTOLu_PxdjaE"}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"if type(train[\"target\"].iloc[0]) is not np.int64:\n    raise RuntimeError('Train backup not loaded properly')\nif(IS_CLASS_MODE_BINARY):\n    train['target'] = train['target'].apply(str)\n    if type(train[\"target\"].iloc[0]) is not str:\n        raise RuntimeError('Train backup not loaded properly')","metadata":{"execution":{"iopub.status.busy":"2021-08-22T07:59:56.299772Z","iopub.execute_input":"2021-08-22T07:59:56.300303Z","iopub.status.idle":"2021-08-22T07:59:56.318119Z","shell.execute_reply.started":"2021-08-22T07:59:56.300263Z","shell.execute_reply":"2021-08-22T07:59:56.317154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# reduce amount of data when running on a cpu\nif DEVICE == 'CPU':\n    raise RuntimeError('Running on CPU')\n    print(\"rows in train\", train.shape[0])\n    print(\"rows in test\", test.shape[0])\n    print(\"reduce the amount of data because of cpu a runtime\")\n    # take 30% of the available data\n    train = train.sample(int(train.shape[0] * 0.3))\n    EPOCHS = 5\n    SAVE_OUTPUT = False\n    print(\"rows in train\", train.shape[0])\n    print(\"rows in test\", test.shape[0])\n\n","metadata":{"id":"l40Yikt8djaE","execution":{"iopub.status.busy":"2021-08-22T07:59:56.320061Z","iopub.execute_input":"2021-08-22T07:59:56.320592Z","iopub.status.idle":"2021-08-22T07:59:56.331066Z","shell.execute_reply.started":"2021-08-22T07:59:56.320525Z","shell.execute_reply":"2021-08-22T07:59:56.33009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Balance the dataset /UNDERSAMPLING\n\nBecause we have a highly imbalanced dataset we need to balance it.","metadata":{"id":"bamRcA-xdjaJ"}},{"cell_type":"code","source":"if DO_UNDERSAMPLING:\n\n    print(train[train.target == POSITIVE_CLASS].shape, \"positive in train\")\n    print(train[train.target == NEGATIVE_CLASS].shape, \"negative in train\")\n    # 1 means 50 / 50 => equal amount of positive and negative cases in Training\n    # 4 = 20%; 8 = ~11%; 12 = ~8%\n    balance = 1\n    p_inds = train[train.target == POSITIVE_CLASS].index.tolist()\n    np_inds = train[train.target == NEGATIVE_CLASS].index.tolist()\n\n    np_sample = random.sample(np_inds, balance * len(p_inds))\n    train = train.loc[p_inds + np_sample]\n    # print(\"Samples in train\", train['target'].sum()/len(train))\n    # print(\"Remaining rows in train set\", len(train))\nelse:\n    print(\"No undersampling\")\n\nprint(train[train.target == POSITIVE_CLASS].shape, \"positive in train\")\nprint(train[train.target == NEGATIVE_CLASS].shape, \"negative in train\")","metadata":{"id":"3Ayqx5OTdjaJ","outputId":"4deb5bc6-f858-4c91-fb14-9c33f3a05d66","execution":{"iopub.status.busy":"2021-08-22T07:59:56.332678Z","iopub.execute_input":"2021-08-22T07:59:56.33318Z","iopub.status.idle":"2021-08-22T07:59:56.388538Z","shell.execute_reply.started":"2021-08-22T07:59:56.333142Z","shell.execute_reply":"2021-08-22T07:59:56.387575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Patient Overlap\n\nImportant to note is that there are patients with multiple images taken in both train and test datasets.\nWe, therefore, need to check that the same patient images do not appear in the training and test set.\n","metadata":{"id":"1lWAdFggdjaK"}},{"cell_type":"code","source":"print(\"Max number of images from one patient in the train set:\", np.max(train.patient_id.value_counts()))\nprint(\"Max number of images from one patient in the test set:\", np.max(test.patient_id.value_counts()))\n\n# get the unique patient ids from the test and training set\nids_train = set(train.patient_id.values)\nids_test = set(test.patient_id.values)\n\nprint(\"There are\", len(ids_train), \"unique patients in the training set\")\nprint(\"There are\", len(ids_test), \"unique patients in the test set\")\n\n# Identify patient overlap by looking at the intersection between the sets\npatient_overlap = list(ids_train.intersection(ids_test))\nn_overlap = len(patient_overlap)\nprint(\"There are\", n_overlap, \"patients in both the training and test sets\")","metadata":{"id":"IcKg60KjdjaK","outputId":"4183b2fc-fbe8-4c97-da18-704149b89861","execution":{"iopub.status.busy":"2021-08-22T08:07:35.292792Z","iopub.execute_input":"2021-08-22T08:07:35.293264Z","iopub.status.idle":"2021-08-22T08:07:35.314531Z","shell.execute_reply.started":"2021-08-22T08:07:35.293221Z","shell.execute_reply":"2021-08-22T08:07:35.313396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\" Helper function to create a train and a validation dataset\n\n    Parameters:\n    df (dataframe): The dataframe to split\n    test_size (int): Size of the validation set\n    classToPredict: The target column\n\n    Returns:\n    train_data (dataframe)\n    val_data (dataframe)\n\"\"\"\ndef create_splits(df, test_size, classToPredict):\n    train_data, val_data = train_test_split(df,  test_size = test_size, random_state = SEED, stratify = df[classToPredict])\n    return train_data, val_data\n\n\"\"\" Helper function to plot the history of a tensorflow model\n\n    Parameters:\n        history (history object): The history from a tf model\n        timestamp (string): The timestamp of the function execution\n\n    Returns:\n        Null\n\"\"\"\ndef save_history(history, timestamp):\n    f = plt.figure()\n    f.set_figwidth(15)\n\n    f.add_subplot(1, 2, 1)\n    plt.plot(history['val_loss'], label='val loss')\n    plt.plot(history['loss'], label='train loss')\n    plt.legend()\n    plt.title(\"Modell Loss\")\n\n    f.add_subplot(1, 2, 2)\n    plt.plot(history['val_accuracy'], label='val accuracy')\n    plt.plot(history['accuracy'], label='train accuracy')\n    plt.legend()\n    plt.title(\"Modell Accuracy\")\n\n    if SAVE_OUTPUT:\n        length = len(history[\"loss\"])-1\n        metrics = [\"loss\", \"accuracy\",\"auc\",\"val_loss\", \"val_accuracy\",\"val_auc\"]\n        f = open(base_output_path + \"2finalResults.txt\", \"a\")\n        for metric in metrics:\n            metricValue = round(history[metric][length],4)\n            f.write(metric + \":\" + str(metricValue) + (\"\\n\\n\" if metric == \"auc\" else \"\\n\"))\n        f.close()\n        plt.savefig(base_output_path + \"2history.png\")\n        with open(base_output_path + \"2history.json\", 'w') as f:\n            json.dump(history, f)\n            \n            \n\"\"\" Helper function to plot the auc curve\n\n    Parameters:\n        t_y (array): True binary labels\n        p_y (array): Target scores\n\n    Returns:\n        Null\n\"\"\"\ndef plot_auc(t_y, p_y):\n    fpr, tpr, thresholds = roc_curve(t_y, p_y, pos_label=1)\n    fig, c_ax = plt.subplots(1,1, figsize = (8, 8))\n    c_ax.plot(fpr, tpr, label = '%s (AUC:%0.2f)'  % ('Target', auc(fpr, tpr)))\n    c_ax.plot([0, 1], [0, 1], color='navy', lw=1, linestyle='--')\n    c_ax.legend()\n    c_ax.set_xlabel('False Positive Rate')\n    c_ax.set_ylabel('True Positive Rate')\n    plt.savefig(base_output_path + \"5auc.png\")","metadata":{"id":"bIdtddiJdjaL","execution":{"iopub.status.busy":"2021-08-22T08:07:35.317373Z","iopub.execute_input":"2021-08-22T08:07:35.317789Z","iopub.status.idle":"2021-08-22T08:07:35.337199Z","shell.execute_reply.started":"2021-08-22T08:07:35.317743Z","shell.execute_reply":"2021-08-22T08:07:35.335885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data augmentation","metadata":{}},{"cell_type":"code","source":"def customPreprocess(image):\n    if (PREPRO_ROTATION):\n        image = np.rot90(image, np.random.choice([-1, 0, 1, 2]))\n    if (PREPRO_BLUR and (bool(random.getrandbits(1)))):\n        image = cv2.blur(image,(3,3))\n    return image\n\n\n \n# datagen = ImageDataGenerator(preprocessing_function= blur)\ndef get_training_gen(df):\n    #chanel shift nos lo quitamos y probar a reducr para acortar tiempo de entrenamiento.\n    train_idg = ImageDataGenerator(\n        rescale = 1 / 255.0,\n        horizontal_flip = True,\n        vertical_flip = True,\n        brightness_range = [PREPRO_BRIGHTNESS_LOW,1],\n        zoom_range = [PREPRO_ZOOM_LOW,1],\n        fill_mode='nearest',\n        preprocessing_function=customPreprocess\n    )\n\n    train_gen = train_idg.flow_from_dataframe(\n        seed=SEED,\n        dataframe=df,\n        directory=None,\n        x_col='image_path',\n        y_col='target',\n        class_mode=CLASS_MODE,\n        shuffle=True,\n        target_size=IMG_SIZE,\n        batch_size=BATCH_SIZE,\n        validate_filenames = False\n    )\n\n    return train_gen\n\n\"\"\" Factory function to create a validation image data generator\n\nParameters:\n    df (dataframe): Validation dataframe \n\nReturns:\n    Image Data Generator function\n\"\"\"\ndef get_validation_gen(df):\n    ## prepare images for validation\n    val_idg = ImageDataGenerator(rescale=1. / 255.0)\n    val_gen = val_idg.flow_from_dataframe(\n        seed=SEED,\n        dataframe=df,\n        directory=None,\n        x_col='image_path',\n        y_col='target',\n        class_mode=CLASS_MODE,\n        shuffle=False,\n        target_size=IMG_SIZE,\n        batch_size=BATCH_SIZE,\n        validate_filenames = False\n    )\n\n    return val_gen","metadata":{"execution":{"iopub.status.busy":"2021-08-22T08:07:35.339415Z","iopub.execute_input":"2021-08-22T08:07:35.339828Z","iopub.status.idle":"2021-08-22T08:07:35.354785Z","shell.execute_reply.started":"2021-08-22T08:07:35.339776Z","shell.execute_reply":"2021-08-22T08:07:35.353842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Images returned from the ImageDataGenerator","metadata":{"id":"Y_EECRkFdjaN"}},{"cell_type":"code","source":"if SAVE_OUTPUT:\n    train_gen = get_training_gen(train)\n    t_x, t_y = next(train_gen)\n    fig, m_axs = plt.subplots(4, 4, figsize = (16, 16))\n    for (c_x, c_y, c_ax) in zip(t_x, t_y, m_axs.flatten()):\n        c_ax.imshow(c_x, cmap = 'bone')\n        if c_y == \"1\": \n            c_ax.set_title(str(c_y) + \"-MALIGNANT\")\n        else:\n            c_ax.set_title(str(c_y) + \"-BENIGN\")\n        c_ax.axis('off')\n\n\n    plt.savefig(base_output_path + \"1dataAug.png\")","metadata":{"tags":[],"id":"GeRvqJ-qdjaO","outputId":"08f1a46e-540f-46b4-de53-07d7fe435219","execution":{"iopub.status.busy":"2021-08-22T08:07:35.356421Z","iopub.execute_input":"2021-08-22T08:07:35.356934Z","iopub.status.idle":"2021-08-22T08:07:38.891984Z","shell.execute_reply.started":"2021-08-22T08:07:35.356894Z","shell.execute_reply":"2021-08-22T08:07:38.890791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The Image Data Generator function returns these transformed images.\n\nThe Keras ImageDataGenerator class works by:\n- Accepting a batch of images used for training.\n- Taking this batch and applying a series of random transformations to each image in the batch (including random rotation, resizing, shearing, etc.).\n- Replacing and returning the original batch with the new, randomly transformed batch.\n\nSource: https://www.pyimagesearch.com/2019/07/08/keras-imagedatagenerator-and-data-augmentation/","metadata":{}},{"cell_type":"markdown","source":"## Transfer Learning\n\nConventional machine learning and deep learning algorithms, so far, have been traditionally designed to work in isolation. These algorithms are trained to solve specific tasks. The models have to be rebuilt from scratch once the feature-space distribution changes. Transfer learning is the idea of overcoming the isolated learning paradigm and utilizing knowledge acquired for one task to solve related ones. ","metadata":{"id":"67EjcGWPdjaO"}},{"cell_type":"markdown","source":"\n![Transfer Learning](https://github.com/SaschaMet/melanoma-classification/blob/master/images/transfer-learning.png?raw=1)\n ","metadata":{"id":"fkqvRu-0djaO"}},{"cell_type":"markdown","source":"Traditional learning is isolated and occurs purely based on specific tasks, datasets, and training separate isolated models on them. No knowledge is retained, which can be transferred from one model to another. In transfer learning, you can leverage knowledge (features, weights, etc.) from previously trained models for training newer models and even tackle problems like having less data for the more recent task.","metadata":{"id":"g-vCqePSD4iI"}},{"cell_type":"markdown","source":"**Fine Tuning Off-the-shelf Pre-trained Models**\n\nThis is a more involved technique, where we do not just replace the final layer (for classification/regression), but we also selectively retrain some of the previous layers. \n\n\n![Transfer Learning](https://miro.medium.com/max/700/1*BBZGHtI_vhDBeqsIbgMj1w.png)\n \n\n","metadata":{"id":"oLBLoI_mEp3q"}},{"cell_type":"markdown","source":"Source: https://towardsdatascience.com/a-comprehensive-hands-on-guide-to-transfer-learning-with-real-world-applications-in-deep-learning-212bf3b2f27a","metadata":{"id":"TXTzfRx8EmSV"}},{"cell_type":"code","source":"def load_pretrained_model():\n\n    # freeze the first 15 layers of the base model. All other layers are trainable.\n    for layer in base_model.layers[0:15]:\n        layer.trainable = BASE_MODEL_TRAINABLE\n\n    for idx, layer in enumerate(base_model.layers):\n        print(\"layer\", idx + 1, \":\", layer.name, \"is trainable:\", layer.trainable)\n\n    return base_model\n\ndef create_model():\n    print(\"create model\")\n    model = Sequential()\n    model.add(load_pretrained_model())  \n    # Add a flatten layer to prepare the ouput of the cnn layer for the next layers\n    model.add(layers.Flatten())\n    # Add a dense (aka. fully-connected) layer. \n    model.add(layers.Dense(128, activation='relu'))\n    # Add a dropout-layer which may prevent overfitting and improve generalization ability to unseen data.\n    model.add(layers.Dropout(0.3))\n\n    model.add(layers.Dense(64, activation='relu'))\n    model.add(layers.Dropout(0.3))\n\n    model.add(layers.Dense(32, activation='relu'))\n\n    # Use the Sigmoid activation function for binary predictions, softmax for n-classes\n    model.add(layers.Dense(OUTPUT_NEURONS, activation='sigmoid'))\n    return model\n\nmodel = create_model()\nmodel.summary()","metadata":{"id":"5aWRaNHUdjaO","execution":{"iopub.status.busy":"2021-08-22T08:07:38.894944Z","iopub.execute_input":"2021-08-22T08:07:38.895293Z","iopub.status.idle":"2021-08-22T08:07:42.906367Z","shell.execute_reply.started":"2021-08-22T08:07:38.895258Z","shell.execute_reply":"2021-08-22T08:07:42.905644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"callback_list = []\n\n# if the model does not improve for 10 epochs, stop the training\nstop_early = EarlyStopping(monitor='val_loss', mode='auto', patience=10)\ncallback_list.append(stop_early)\n\n# if the output of the model should be saved, create a checkpoint callback function\nif SAVE_OUTPUT:\n    # set the weight path for saving the model\n    weight_path = base_output_path + \"3model.hdf5\"\n    # create the model checkpoint callback to save the model wheights to a file\n    checkpoint = ModelCheckpoint(\n        weight_path,\n        save_weights_only=True,\n        verbose=VERBOSE_LEVEL,\n        save_best_only=True,\n        monitor='val_loss',\n        overwrite=True,\n        mode='auto',\n    )\n    # append the checkpoint callback to the callback list\n    callback_list.append(checkpoint)","metadata":{"id":"xWdRhc_mdjaQ","execution":{"iopub.status.busy":"2021-08-22T08:07:42.908071Z","iopub.execute_input":"2021-08-22T08:07:42.908411Z","iopub.status.idle":"2021-08-22T08:07:42.916905Z","shell.execute_reply.started":"2021-08-22T08:07:42.908377Z","shell.execute_reply":"2021-08-22T08:07:42.915451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model training","metadata":{}},{"cell_type":"code","source":"# create a training and validation dataset from the train df\ntrain_df, val_df = create_splits(train, 0.2, 'target')\n\nprint(\"rows in train_df\", train_df.shape[0])\nprint(\"rows in val_df\", val_df.shape[0])\n\nprint(train_df.dtypes)\nprint(val_df.dtypes)\n\n# because we do not need the target column anymore we can drop it\n# train_df.drop(['target'], axis=1, inplace=True)\n# val_df.drop(['target'], axis=1, inplace=True)\n# print(train_df.dtypes)\n# print(val_df.dtypes)\n\n# call the generator functions\ntrain_gen = get_training_gen(train_df)\nval_gen = get_validation_gen(val_df)\nvalX, valY = val_gen.next()","metadata":{"id":"zxj3bFJPdjaR","outputId":"fbc3976a-1fbb-40fc-90a2-dd521dc63d7f","execution":{"iopub.status.busy":"2021-08-22T08:07:42.918488Z","iopub.execute_input":"2021-08-22T08:07:42.918862Z","iopub.status.idle":"2021-08-22T08:07:43.756544Z","shell.execute_reply.started":"2021-08-22T08:07:42.918823Z","shell.execute_reply":"2021-08-22T08:07:43.755718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nclass_weights = np.array([1.0,1.0])\n#Probar undersampling calssweights y tal.\n\nif not DO_UNDERSAMPLING:\n    print(\"Calculating class weights (no undersampling)\")\n    #quizás auemntar epochs pero complicado con 1000\n    testY = np.array(train_df['target'])\n    class_weights = class_weight.compute_class_weight('balanced',np.unique(testY), testY)\nelse:\n    print(\"No class weights (previously undersampled)\")\n\nclass_weights = {i : class_weights[i] for i in range(2)}\n\nprint(class_weights)","metadata":{"execution":{"iopub.status.busy":"2021-08-22T08:07:43.757859Z","iopub.execute_input":"2021-08-22T08:07:43.758226Z","iopub.status.idle":"2021-08-22T08:07:43.769416Z","shell.execute_reply.started":"2021-08-22T08:07:43.758181Z","shell.execute_reply":"2021-08-22T08:07:43.768292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#sE CENTRA EN POSIBLES PREPROESADOS Y TAL, NO TANTO EN MODELO IDEAL PERFECTO CON LEARNING RATE Y TAL Y CUAL\nprint(\"CLASSMODE:\", CLASS_MODE)\nprint(\"DO_UNDERSAMPLING\", DO_UNDERSAMPLING)\nprint(\"BATCH_SIZE: \", BATCH_SIZE)\nprint(\"EPOCHS: \", EPOCHS)\nprint(\"PREPRO_ROTATION\", PREPRO_ROTATION)\nprint(\"PREPRO_BLUR\", PREPRO_BLUR)\nprint(\"LEARNING_RATE\", LEARNING_RATE)\n\n\nOPTIMIZER = Adam(lr=LEARNING_RATE) #Dejar esto que es el que mejor funcionar\nLOSS = 'binary_crossentropy'\nMETRICS = [\n    'accuracy', \n    'AUC'\n] \n\nmodel.compile(\n    loss=LOSS,\n    metrics=METRICS,\n    optimizer=OPTIMIZER,\n)\nif FAST_RUN:\n    EPOCHS = 3\n# when on a cpu, do not save the model data\nif DEVICE == 'CPU':\n    print(\"fit model on cpu\")\n    history = model.fit(\n        train_gen, \n        epochs=EPOCHS, \n        class_weight = class_weights,\n        verbose=VERBOSE_LEVEL,\n        validation_data=(valX, valY)\n    )\nelse:\n    print(\"fit model on gpu\")\n    history = model.fit(\n        train_gen, \n        epochs=EPOCHS, \n        class_weight = class_weights,\n        verbose=VERBOSE_LEVEL,\n        callbacks=callback_list, \n        validation_data=(valX, valY),\n    )","metadata":{"tags":[],"id":"3JvcUnNLdjab","outputId":"ca64560f-232f-4206-c64a-c4674c5035da","execution":{"iopub.status.busy":"2021-08-22T08:07:43.771498Z","iopub.execute_input":"2021-08-22T08:07:43.771987Z","iopub.status.idle":"2021-08-22T08:13:47.961519Z","shell.execute_reply.started":"2021-08-22T08:07:43.771948Z","shell.execute_reply":"2021-08-22T08:13:47.960695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Evaluation","metadata":{"id":"VT9WHX7ze3sz"}},{"cell_type":"code","source":"# plot model history\nsave_history(history.history, timestamp)","metadata":{"id":"Bn1YGzsedjac","outputId":"4923d9ec-9d98-4e40-c1eb-3d927c6cda9e","execution":{"iopub.status.busy":"2021-08-22T08:13:47.963463Z","iopub.execute_input":"2021-08-22T08:13:47.963745Z","iopub.status.idle":"2021-08-22T08:13:48.394403Z","shell.execute_reply.started":"2021-08-22T08:13:47.963716Z","shell.execute_reply":"2021-08-22T08:13:48.393422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the accuracy plot, we can see that the model stops learning after epoch 22. We can also see that the model has not yet over-learned the training dataset. We can see that the model has comparable performance on both train and validation datasets from the loss plot. The model achieved the lowest loss at the 19th epoch.\n\nHowever, the plots suggest that our model has difficulty generalizing, as the validation curves vary widely in some cases.","metadata":{"id":"0NlzfWYk0_f_"}},{"cell_type":"code","source":"# plot the auc\ny_t = [] # true labels\ny_p = [] # predictions\n\n# iterate over the validation df and make a prediction for each image\n# for i in tqdm(range(val_df.shape[0])):\nrangeValue = val_df.shape[0] if not FAST_RUN else 50\nfor i in range(rangeValue):\n    y_real = val_df.iloc[i].target\n    y_real_int = int(y_real)\n    image_path = val_df.iloc[i].image_path\n\n    img = keras.preprocessing.image.load_img(image_path, target_size=IMG_SIZE)\n    img = keras.preprocessing.image.img_to_array(img)\n    img = img / 255\n    img_array = tf.expand_dims(img, 0)\n    y_pred = model.predict(img_array)\n    y_pred_num = round(y_pred[0][0],2)\n    y_pred_class = int(round(y_pred_num, 0))\n    #print(\"Real: \", y_real, \"-> pred: \", y_pred_num, \"class\", y_pred_class)\n    y_t.append(y_real_int)\n    y_p.append(y_pred_class)\n\nplot_auc(y_t, y_p)","metadata":{"id":"7PGfEYGqdjac","outputId":"62f7716d-73f8-4093-9fcf-f6fd7686725f","execution":{"iopub.status.busy":"2021-08-22T08:13:48.396081Z","iopub.execute_input":"2021-08-22T08:13:48.396546Z","iopub.status.idle":"2021-08-22T08:14:13.228531Z","shell.execute_reply.started":"2021-08-22T08:13:48.396497Z","shell.execute_reply":"2021-08-22T08:14:13.227741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"AUC - ROC curve is a performance measurement for the classification problem at various threshold settings. ROC is a probability curve, and AUC represents the degree or measure of separability. It tells how much the model is capable of distinguishing between classes. Higher the AUC, the better the model is at predicting 0s as 0s and 1s as 1s. By analogy, the Higher the AUC, the better the model is at distinguishing between patients with the disease and no disease. The ROC curve is plotted with TPR against the FPR, where TPR is on the y-axis and FPR is on the x-axis.\n\nSource: https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5","metadata":{"id":"0xTrPH_D0_f_"}},{"cell_type":"markdown","source":"### F1 Score Calculation\n\nThe F1 score is the harmonic mean of precision and recall. In a statistical analysis of binary classification, the F-score is a measure of a test's accuracy. It is calculated from the precision and recall of the test, where the precision is the number of correctly identified positive results divided by the number of all positive outcomes, including those not identified correctly. The recall is the number of correctly identified positive results divided by the number of all samples that should have been identified as positive.\n\nThe highest possible value of an F-score is 1, indicating perfect precision and recall, and the lowest potential value is 0 if either the precision or the recall is zero.\n\nSource: https://en.wikipedia.org/wiki/F-score","metadata":{"id":"uINcuIl7oN6y"}},{"cell_type":"code","source":"\"\"\" Helper function to calculate the F1 Score\n\n    Parameters:\n        prec (int): precision\n        recall (int): recall\n\n    Returns:\n        f1 score (int)\n\"\"\"\ndef calc_f1(prec, recall):\n    return 2*(prec*recall)/(prec+recall) if recall and prec else 0\n\n# calculate the precision, recall and the thresholds\nprecision, recall, thresholds = precision_recall_curve(y_t, y_p)\n\n# calculate the f1 score\nf1score = [calc_f1(precision[i],recall[i]) for i in range(len(thresholds))]\n\n# get the index from the highest f1 score\nidx = np.argmax(f1score)\n\n# get the precision, recall, threshold and the f1score\nprecision = round(precision[idx], 4)\nrecall = round(recall[idx], 4)\n# threshold = round(thresholds[idx], 4)\nf1score = round(f1score[idx], 4)\n\nprint('Precision:', precision)\nprint('Recall:', recall)\n# print('Threshold:', threshold)\nprint('F1 Score:', f1score)","metadata":{"id":"4M-vmev8n2EQ","outputId":"90da6820-5de2-4677-9747-c98feed85dcd","execution":{"iopub.status.busy":"2021-08-22T08:14:13.230722Z","iopub.execute_input":"2021-08-22T08:14:13.231013Z","iopub.status.idle":"2021-08-22T08:14:13.245701Z","shell.execute_reply.started":"2021-08-22T08:14:13.230985Z","shell.execute_reply":"2021-08-22T08:14:13.244205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create a confusion matrix\ncm =  confusion_matrix(y_t, y_p)\ncm","metadata":{"id":"asa6b_y9AD4y","outputId":"5f807a95-12de-448f-bc2c-583298d614a2","execution":{"iopub.status.busy":"2021-08-22T08:14:13.248559Z","iopub.execute_input":"2021-08-22T08:14:13.249177Z","iopub.status.idle":"2021-08-22T08:14:13.259042Z","shell.execute_reply.started":"2021-08-22T08:14:13.249133Z","shell.execute_reply":"2021-08-22T08:14:13.258235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\" Helper function to plot a confusion matrix\n\n    Parameters:\n        cm (confusion matrix)\n\n    Returns:\n        Null\n\"\"\"\ndef plot_confusion_matrix(cm, labels):\n    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n    plt.title('Confusion Matrix')\n    plt.colorbar()\n    tick_marks = np.arange(len(labels))\n    plt.xticks(tick_marks, labels, rotation=55)\n    plt.yticks(tick_marks, labels)\n\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], 'd'), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.tight_layout()\n    if SAVE_OUTPUT:\n        plt.savefig(base_output_path + \"4confMatrix.png\")\n\ncm_plot_label =['benign', 'malignant']\nplot_confusion_matrix(cm, cm_plot_label)","metadata":{"id":"YyiE5vMSzIpW","outputId":"73ce3f0a-a4dc-4e33-e0a5-13759dee17d7","execution":{"iopub.status.busy":"2021-08-22T08:14:13.260251Z","iopub.execute_input":"2021-08-22T08:14:13.260569Z","iopub.status.idle":"2021-08-22T08:14:13.488843Z","shell.execute_reply.started":"2021-08-22T08:14:13.260538Z","shell.execute_reply":"2021-08-22T08:14:13.487837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The model predicted 191 images correclty, but failed on 43.","metadata":{"id":"M3t77b-A0_gA"}},{"cell_type":"markdown","source":"## Inference","metadata":{"id":"iV0hJPkxe6ct"}},{"cell_type":"code","source":"image_path = test.iloc[0].image_path\n# Show a prediction for a random image\nimage_path = test.sample().iloc[0].image_path\nimg = keras.preprocessing.image.load_img(image_path, target_size=IMG_SIZE)\nimg = keras.preprocessing.image.img_to_array(img)\nimg = img / 255\nimg_array = tf.expand_dims(img, 0)\n\ny_pred = model.predict(img_array)\ny_pred_num = round(y_pred[0][0],2)\n    \nprediction = y_pred_num\nprint(\"Chance of being malignant: {:.2f} %\".format(prediction))\n\nfinding = \"Diagnosis: BENIGN\"\nif not prediction < 0.5:\n    finding = \"Diagnosis: MALIGNANT\"\n\nx = plt.figure(figsize=(5,5))\nx = plt.imshow(img)\nx = plt.title(finding)\nx = plt.axis(\"off\")","metadata":{"id":"iHP7Lb3Odjac","outputId":"96bd8ef3-d03c-445d-ea27-d267b8147f1d","execution":{"iopub.status.busy":"2021-08-22T08:14:13.49088Z","iopub.execute_input":"2021-08-22T08:14:13.491284Z","iopub.status.idle":"2021-08-22T08:14:13.838705Z","shell.execute_reply.started":"2021-08-22T08:14:13.491239Z","shell.execute_reply":"2021-08-22T08:14:13.837996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Discussion\n\nWe achieved an F1-Score of 0.817. Based on a study from Han SS, Moon IJ, Lim W, et al. \"Keratinocytic Skin Cancer Detection on the Face Using Region-Based Convolutional Neural Network\" the F1-score of a professional dermatologist is 0.835. Compared to this result, our neural network performed slightly worse. It should be noted, however, that only facial skin cancer was considered in this study. The F1-Score of professional dermatologists on a more realistic dataset like this one could therefore differ.\n\n\nSource: https://pubmed.ncbi.nlm.nih.gov/31799995/","metadata":{"id":"kX5wJVU6zMS2"}},{"cell_type":"markdown","source":"<img src=\"https://github.com/SaschaMet/melanoma-classification/blob/master/images/clinical_relevance.png?raw=1\" alt=\"Clinical Relevance\" style=\"width: 600px;\"/>\n\nSource: https://www.udacity.com/course/ai-for-healthcare-nanodegree--nd320","metadata":{"id":"t_-SwTO3OMU0"}},{"cell_type":"markdown","source":"Precision and Recall are of particular interest to the clinical applicability of the model. A model with high precision has increased confidence in a positive result. It is, therefore, better suited in confirming a diagnosis. A model with high Recall, on the other hand, is most confident when the test is negative. Such a model is better used for prioritization tasks (e.g., which lesions should be looked at first).\n\nThe precision of this model is 0.8136. The recall is 0.8205. This neural network should be better suited for prioritization tasks than for confirming a diagnosis. But, because we achieved an F1-Score comparable to professional dermatologists, plus precision and recall are almost the same, the model could also be useful in, e.g., confirming a dermatologist's diagnosis.","metadata":{"id":"Pgj2fLtWOPz7"}},{"cell_type":"markdown","source":"### Cut-Off Thresholds\n\nThe output of our CNN's last layer will output a probability that an image belongs to a given class (target_0 or target_1). Changing the threshold for this classification will transform the true positive, false positive, false negative, and true negative rates. This will, in turn, change the precision and recall of our model. We could, for example, change the threshold so that our precision increases. This, however, has the result that our recall changes and probably even decreases. One metric is optimized at the expense of another. Because of this, we used the F1-Score as a final metric because the F1-Score combines both precision and recall. (It is the harmonic mean of precision and recall.)","metadata":{"id":"qYl9K5mG0_gB"}},{"cell_type":"markdown","source":"### Kaggle Leaderboard\n\nWhen predicting the images from the provided test set, the model achieves a private score of 0.8041 and a public score of 0.8247. This results in a place on the leaderboard at around 2.700.\n\nThe best model on the private leaderboard of the Kaggle competition achieved a score of 0.9490.\n\nAll submissions are evaluated on the area under the ROC curve between the predicted probability and the observed target.","metadata":{"id":"eNoVI-2v0_gB"}},{"cell_type":"markdown","source":"## How to further improve the model\n\nBased on the placement on the leaderboard, you can already see that there is still a lot of potential for optimization. The following possibilities could be addressed, for example:\n\n- Try different pre-trained models: https://keras.io/api/applications\n- Hyperparameter tuning: https://blog.tensorflow.org/2020/01/hyperparameter-tuning-with-keras-tuner.html\n- Get more training data: https://www.kaggle.com/wanderdust/skin-lesion-analysis-toward-melanoma-detection\n- Experiment with different loss functions: https://www.tensorflow.org/addons/api_docs/python/tfa/losses/SigmoidFocalCrossEntropy\n- Improve the data augmentation, e.g. by removing body hair: https://www.kaggle.com/vatsalparsaniya/melanoma-hair-remove","metadata":{"id":"t6iybwN4OiWn"}},{"cell_type":"markdown","source":"However, even with these options, it will be challenging to get a top ranking on the leaderboard. For example, the winning team consists of three Kaggle Grandmasters, all of whom work at NVIDIA. In an interview, they also mentioned that one of their most significant advantages was the abundant resources (e.g., GPUs) they received from NVIDIA.\n\nSource: https://www.youtube.com/watch?v=L1QKTPb6V_I","metadata":{"id":"iHPE_dwq0_gC"}},{"cell_type":"code","source":"if SAVE_OUTPUT:\n    # save the model to a json file\n    model_json = model.to_json()\n    with open(base_output_path + \"3model.json\", \"w\") as json_file:\n        json_file.write(model_json)\n\n    # create the submission.csv file\n    data=[]\n    \n    rangeValue = test.shape[0] if not FAST_RUN else 50\n    for i in tqdm(range(rangeValue)):\n        image_path = test.iloc[i].image_path\n        image_name = test.iloc[i].image_name\n        img = keras.preprocessing.image.load_img(image_path, target_size=IMG_SIZE)\n        img = keras.preprocessing.image.img_to_array(img)\n        img = img / 255\n        img_array = tf.expand_dims(img, 0)\n        \n        \n        y_pred = model.predict(img_array)\n        y_pred_class = int(round(y_pred[0][0],0))\n        data.append([image_name, y_pred_class])\n\n    sub_df = pd.DataFrame(data, columns = ['image_name', 'target']) \n    sub_df.to_csv(base_output + \"submission.csv\", index=False)\n\n    sub_df.head()","metadata":{"id":"xV0j-1PE0_gC","outputId":"84981ccd-312a-4fb7-d05b-89f5ffd9324c","execution":{"iopub.status.busy":"2021-08-22T08:14:13.840251Z","iopub.execute_input":"2021-08-22T08:14:13.840743Z","iopub.status.idle":"2021-08-22T08:14:25.928226Z","shell.execute_reply.started":"2021-08-22T08:14:13.840703Z","shell.execute_reply":"2021-08-22T08:14:25.927266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shutil.make_archive(\"/kaggle/working/\" + experiment_id, 'zip', \"/kaggle/working/\" + experiment_id)\n\nshutil.rmtree('/kaggle/working/' + experiment_id)","metadata":{"execution":{"iopub.status.busy":"2021-08-22T08:14:25.92969Z","iopub.execute_input":"2021-08-22T08:14:25.930122Z","iopub.status.idle":"2021-08-22T08:14:28.164947Z","shell.execute_reply.started":"2021-08-22T08:14:25.930082Z","shell.execute_reply":"2021-08-22T08:14:28.164152Z"},"trusted":true},"execution_count":null,"outputs":[]}]}